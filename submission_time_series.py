# -*- coding: utf-8 -*-
"""Submission Time Series V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_-lOneFOd_7t9FTZnAaAq8gfTTqoJY5O

Nama : Amrul Fadhil Yofan

Dataset : https://www.kaggle.com/datasets/emmanuelfwerr/london-weather-data

Source : Kaggle
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM,Dense,Embedding,Bidirectional,Dropout,BatchNormalization
from tensorflow.keras.models import Sequential
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
import keras
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import EarlyStopping

"""## Import Dataset dari Kaggle"""

!pip install -q kaggle

# Upload kaggle.json secara manual
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# Download dataset
!kaggle datasets download -d emmanuelfwerr/london-weather-data

# Unzip dan lihat list dataset
!mkdir weather
!unzip london-weather-data.zip -d weather
!ls weather

# Import data
df = pd.read_csv('weather/london_weather.csv')
df.head()

df.info()

"""## Analisis Data dan Cleaning Data"""

# Cek ukuran data
df.shape

"""Tujuan submission ini adalah untuk memprediksi suhu rata-rata cuaca di kota London (tahun 1979-2020) dengan menggunakan time series."""

# Cek data kosong
df.isnull().sum()

# Reduksi data (Mengambil fitur yang dibutuhkan)
df_red = df[["date","max_temp","min_temp","mean_temp"]]
df_red

df_red['date'] = df_red['date'].astype(str)
df_red.info()

# Mengubah tipe data pada fitur "date" menjadi datetime
df_red['date'] =  df_red['date'].str.strip().str[0:4] + '-' + df_red['date'].str.strip().str[4:6] + '-' + df_red['date'].str.strip().str[6:8]

df_red["date"] = pd.to_datetime(df_red["date"])
df_red

df_red.info()

# Cek baris yang memuat data kosong
min_temp_empty = df_red[df_red['min_temp'].isna()]
max_temp_empty = df_red[df_red['max_temp'].isna()]
min_temp_empty

max_temp_empty

# Mengisi data min_temp dan max_temp kosong dengan menggunakan rata-rata hari sebelumnya dan sesudahnya
for i in range(len(df_red)):
  if df_red['min_temp'].isna()[i] == True:
    df_red['min_temp'][i] =  (df_red['min_temp'][i+1] + df_red['min_temp'][i-1])/2

for i in range(len(df_red)):
  if df_red['max_temp'].isna()[i] == True:
    df_red['max_temp'][i] =  (df_red['max_temp'][i+1] + df_red['max_temp'][i-1])/2

# Mengisi data mean_temp dengan menghitung rata-rata min_temp dan max_temp
for i in range(len(df_red)):
  if df_red['mean_temp'].isna()[i] == True:
    df_red['mean_temp'][i] =  (df_red['min_temp'][i] + df_red['max_temp'][i])/2

df_red.info()

# Dipilih mean_temp untuk proses modelling selanjutnya
new_df = df_red.drop(columns=['min_temp','max_temp'])
# new_df = new_df[731:]
new_df

plt.plot(new_df['date'], new_df['mean_temp'])
plt.xlabel('Tahun')
plt.ylabel('Suhu Rata-Rata ($^\circ$C)')
plt.show()

"""## Normalisasi dan Split Data"""

set_df = new_df.set_index('date')
set_df

# Normalisasi
scaler = MinMaxScaler(feature_range = (0, 1))
df_scale = scaler.fit_transform(set_df)
plt.plot(df_scale)

#Split data
train_size = int(len(df_scale)*0.8)
test_size = len(df_scale) - train_size
x_data, y_data = df_scale[0: train_size, :], df_scale[train_size: len(df_scale), :1]
print(train_size, test_size)

def dataset(dataset, time_step=1):
    X, Y = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]
        X.append(a)
        Y.append(dataset[i + time_step, 0])
    return np.array(X), np.array(Y)

x_train, y_train = dataset(x_data, 64)
x_test, y_test = dataset(y_data, 64)

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

print(x_train)

print(x_test)

print(y_train)
print(y_test)

"""## Modelling"""

model = Sequential([
    LSTM(128, return_sequences = True, input_shape=(64, 1)),
    LSTM(64, return_sequences = True),
    Bidirectional(LSTM(64)),
    BatchNormalization(),
    Dropout(0.2),
    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dropout(0.2),
    Dense(16, activation="relu"),
    Dropout(0.3),
    Dense(8, activation="relu"),
    Dropout(0.2),
    Dense(1, activation='linear')
])

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    threshold_mae = (df_scale.max() - df_scale.min()) * 10/100
    if(logs.get('mae')<threshold_mae and logs.get('val_mae')<threshold_mae):
      self.model.stop_training = True
      print("\n MAE < 10% skala data sehingga iterasi dihentikan")
callbacks = myCallback()

history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 50, batch_size = 128, callbacks = callbacks, verbose = 1)

# Grafik Loss dan Accuracy

train_accuracy = history.history['mae']
val_accuracy = history.history['val_mae']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Accuracy
axes[0].plot(train_accuracy, label='Train MAE')
axes[0].plot(val_accuracy, label='Validation MAE')
axes[0].set_title('MAE Model')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend(loc='upper right')

# Loss
axes[1].plot(train_loss, label='Train Loss')
axes[1].plot(val_loss, label='Validation Loss')
axes[1].set_title('Loss Model')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend(loc='upper right')

plt.tight_layout()
plt.show()

